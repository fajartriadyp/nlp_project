# -*- coding: utf-8 -*-
"""Untitled9.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Mc82U4cLpYu2lx0vXP7OSvlNKMbLnEMr
"""

"""
Tugas Kuliah NLP UAS
Social Media Sentiment Analysis to Predict Cryptocurrency Asset Price Volatility

Penulis: Fajar Triady Putra
Fakultas Sains dan Teknologi, Universitas Al Azhar Indonesia
"""

# ==============================================================================
# SETUP DAN MOUNTING GOOGLE DRIVE
# ==============================================================================
from google.colab import drive
import os

try:
    drive.mount('/content/drive')
    print("✅ Google Drive berhasil terhubung.")
except:
    print("⚠️ Gagal menghubungkan Google Drive atau sudah terhubung.")

# Set environment variables
os.environ['CUDA_LAUNCH_BLOCKING'] = '1'

# ==============================================================================
# FASE 1: DATA PREPROCESSING DAN EKSPLORASI
# ==============================================================================
print("=" * 60)
print("FASE 1: DATA PREPROCESSING DAN EKSPLORASI")
print("=" * 60)

# Import libraries yang diperlukan
from datasets import load_dataset
import pandas as pd
import re
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import classification_report, f1_score
import matplotlib.pyplot as plt
import seaborn as sns

# 1.1 Load Dataset
print("\n📥 Memuat dataset cryptocurrency tweets...")
raw_dataset = load_dataset("StephanAkkerman/financial-tweets-crypto")
df = raw_dataset['train'].to_pandas()

print(f"✅ Dataset berhasil dimuat dengan {len(df)} tweets")
print(f"📊 Kolom yang tersedia: {list(df.columns)}")
print(f"\n📋 Contoh data:")
print(df[['description', 'sentiment']].head())

# 1.2 Data Cleaning dan Preprocessing
print("\n🧹 Melakukan pembersihan data...")

# Hapus baris dengan nilai kosong
df_original_size = len(df)
df.dropna(subset=['description', 'sentiment'], inplace=True)
print(f"📝 Menghapus {df_original_size - len(df)} baris dengan nilai kosong")

# Filter hanya sentimen yang valid
valid_sentiments = ['Bullish', 'Bearish', 'Neutral']
df = df[df['sentiment'].isin(valid_sentiments)]
print(f"📊 Data setelah filtering: {len(df)} tweets")

def clean_tweet(text):
    """
    Fungsi pembersihan teks tweet sesuai metodologi proposal
    """
    if not isinstance(text, str):
        return ""

    # Lowercase conversion
    text = text.lower()

    # Removal of URLs
    text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)

    # Removal of user mentions dan hashtag
    text = re.sub(r'\@\w+|\#','', text)

    # Removal of special characters, hanya menyisakan huruf dan spasi
    text = re.sub(r'[^a-z\s]', '', text)

    # Menghapus spasi berlebih
    text = re.sub(r'\s+', ' ', text).strip()

    return text

# Terapkan fungsi pembersihan
df['cleaned_text'] = df['description'].apply(clean_tweet)

print("\n✅ Pembersihan teks selesai")
print("📋 Contoh data setelah dibersihkan:")
print(df[['description', 'cleaned_text', 'sentiment']].head())

# 1.3 Analisis Distribusi Sentimen
print("\n📊 Distribusi sentimen dalam dataset:")
sentiment_counts = df['sentiment'].value_counts()
print(sentiment_counts)

# Visualisasi distribusi sentimen
plt.figure(figsize=(10, 6))
plt.subplot(1, 2, 1)
sentiment_counts.plot(kind='bar', color=['green', 'red', 'gray'])
plt.title('Distribusi Sentimen Tweets')
plt.xlabel('Sentimen')
plt.ylabel('Jumlah Tweets')
plt.xticks(rotation=45)

plt.subplot(1, 2, 2)
plt.pie(sentiment_counts.values, labels=sentiment_counts.index, autopct='%1.1f%%',
        colors=['green', 'red', 'gray'])
plt.title('Proporsi Sentimen Tweets')

plt.tight_layout()
plt.show()

# ==============================================================================
# FASE 2: MODEL BASELINE (TF-IDF + NAIVE BAYES)
# ==============================================================================
print("\n" + "=" * 60)
print("FASE 2: MODEL BASELINE (TF-IDF + NAIVE BAYES)")
print("=" * 60)

print("🤖 Membangun model baseline menggunakan TF-IDF + Multinomial Naive Bayes...")

# 2.1 Split data menjadi training dan testing
X_train, X_test, y_train, y_test = train_test_split(
    df['cleaned_text'],
    df['sentiment'],
    test_size=0.2,
    random_state=42,
    stratify=df['sentiment']  # Penting untuk data yang tidak seimbang
)

print(f"📊 Data training: {len(X_train)} tweets")
print(f"📊 Data testing: {len(X_test)} tweets")

# 2.2 Buat representasi TF-IDF
print("\n🔢 Membuat representasi TF-IDF...")
vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))
X_train_tfidf = vectorizer.fit_transform(X_train)
X_test_tfidf = vectorizer.transform(X_test)

print(f"✅ TF-IDF matrix shape: {X_train_tfidf.shape}")

# 2.3 Latih model Multinomial Naive Bayes
print("\n🎯 Melatih model Naive Bayes...")
nb_model = MultinomialNB()
nb_model.fit(X_train_tfidf, y_train)

# 2.4 Evaluasi model baseline
print("\n📈 Melakukan prediksi dan evaluasi...")
y_pred_nb = nb_model.predict(X_test_tfidf)
baseline_f1_macro = f1_score(y_test, y_pred_nb, average='macro')

print("\n" + "="*50)
print("HASIL EVALUASI MODEL BASELINE")
print("="*50)
print(classification_report(y_test, y_pred_nb))
print(f"🎯 F1-Score (Macro) Baseline: {baseline_f1_macro:.4f}")
print("="*50)

# ==============================================================================
# FASE 3: FINE-TUNING BERTWEET
# ==============================================================================
print("\n" + "=" * 60)
print("FASE 3: FINE-TUNING BERTWEET")
print("=" * 60)

from datasets import Dataset, Features, ClassLabel, Value
from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer
from huggingface_hub import notebook_login

print("🚀 Memulai fine-tuning BERTweet untuk domain cryptocurrency...")

# 3.1 Setup Hugging Face Authentication
print("\n🔐 Setup autentikasi Hugging Face...")
try:
    notebook_login()
    print("✅ Autentikasi berhasil")
except:
    print("⚠️ Menggunakan token manual...")
    hf_token = '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'

# 3.2 Persiapan Dataset untuk Hugging Face
print("\n📊 Mempersiapkan dataset untuk Hugging Face...")

# Konversi label sentimen menjadi numerik
labels = df['sentiment'].unique().tolist()
label2id = {label: i for i, label in enumerate(labels)}
id2label = {i: label for i, label in enumerate(labels)}
df['label'] = df['sentiment'].map(label2id)

print(f"📝 Mapping label: {label2id}")

# Definisikan schema dataset
features = Features({
    'description': Value(dtype='string'),
    'cleaned_text': Value(dtype='string'),
    'sentiment': Value(dtype='string'),
    'label': ClassLabel(names=labels)
})

# Buat Dataset Hugging Face
clean_df_for_hf = df[['description', 'cleaned_text', 'sentiment', 'label']].reset_index(drop=True)
hf_dataset = Dataset.from_pandas(clean_df_for_hf, features=features)

# Split dataset
train_test_dataset = hf_dataset.train_test_split(test_size=0.2, seed=42, stratify_by_column='label')
train_dataset = train_test_dataset['train']
test_dataset = train_test_dataset['test']

print(f"✅ Dataset HF berhasil dibuat:")
print(f"   - Training: {len(train_dataset)} samples")
print(f"   - Testing: {len(test_dataset)} samples")

# 3.3 Setup Tokenizer dan Model
print("\n🔤 Memuat tokenizer dan model BERTweet...")

# Install dependencies
!pip install emoji==0.6.0 -q

tokenizer = AutoTokenizer.from_pretrained("vinai/bertweet-base")

def tokenize_function(examples):
    """Tokenisasi dengan padding dan truncation"""
    return tokenizer(examples["cleaned_text"], padding="max_length", truncation=True, max_length=128)

# Terapkan tokenisasi
print("🔤 Menerapkan tokenisasi pada dataset...")
tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)
tokenized_test_dataset = test_dataset.map(tokenize_function, batched=True)

# 3.4 Load Model untuk Fine-tuning
print("\n🤖 Memuat model BERTweet untuk fine-tuning...")
model = AutoModelForSequenceClassification.from_pretrained(
    "vinai/bertweet-base",
    num_labels=len(labels),
    id2label=id2label,
    label2id=label2id
)

# 3.5 Setup Training Arguments
output_path = "/content/drive/MyDrive/proyek_nlp_crypto/bertweet-finetuned"

training_args = TrainingArguments(
    output_dir=output_path,
    eval_strategy="epoch",
    save_strategy="epoch",
    learning_rate=3e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=3,
    weight_decay=0.01,
    load_best_model_at_end=True,
    metric_for_best_model="f1",
    logging_dir=f"{output_path}/logs",
    logging_steps=50,
    warmup_steps=100,
)

def compute_metrics(eval_pred):
    """Fungsi untuk menghitung F1-score"""
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)
    return {"f1": f1_score(labels, predictions, average="macro")}

# 3.6 Create Trainer dan Mulai Fine-tuning
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train_dataset,
    eval_dataset=tokenized_test_dataset,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)

print("\n🎯 Memulai proses fine-tuning...")
print("⏳ Proses ini akan memakan waktu beberapa menit...")

trainer.train()

# 3.7 Evaluasi Model Fine-tuned
print("\n📊 Evaluasi model BERTweet yang sudah di-fine-tune...")
eval_results = trainer.evaluate()

print("\n" + "="*60)
print("HASIL EVALUASI MODEL BERTWEET FINE-TUNED")
print("="*60)
print(f"🎯 F1-Score (Macro) BERTweet: {eval_results['eval_f1']:.4f}")
print(f"📈 Peningkatan dari Baseline: {eval_results['eval_f1'] - baseline_f1_macro:.4f}")
print("="*60)

# Simpan model terbaik
trainer.save_model(f"{output_path}/best_model")
print(f"💾 Model terbaik disimpan di: {output_path}/best_model")

# ==============================================================================
# FASE 4: PREDIKSI SENTIMEN PADA SELURUH DATASET
# ==============================================================================
print("\n" + "=" * 60)
print("FASE 4: PREDIKSI SENTIMEN PADA SELURUH DATASET")
print("=" * 60)

from tqdm.auto import tqdm

# 4.1 Load Model Terbaik untuk Prediksi
model_path = f"{output_path}/best_model"
print(f"📂 Memuat model dari: {model_path}")

tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForSequenceClassification.from_pretrained(model_path)

# 4.2 Persiapan Dataset Lengkap
print("\n📊 Mempersiapkan dataset lengkap untuk prediksi...")
raw_dataset = load_dataset("StephanAkkerman/financial-tweets-crypto")
df_full = raw_dataset['train'].to_pandas()
df_full.dropna(subset=['description'], inplace=True)

# Bersihkan teks
df_full['cleaned_text'] = df_full['description'].apply(clean_tweet)

# Konversi ke Dataset HF
full_hf_dataset = Dataset.from_pandas(df_full)

def tokenize_data(examples):
    """Tokenisasi untuk prediksi"""
    return tokenizer(examples['cleaned_text'], truncation=True, padding=True, max_length=128)

tokenized_full_dataset = full_hf_dataset.map(tokenize_data, batched=True)

# 4.3 Prediksi Sentimen
print("\n🔮 Melakukan prediksi sentimen pada seluruh dataset...")
trainer = Trainer(model=model)
raw_predictions = trainer.predict(tokenized_full_dataset)

# Konversi prediksi ke label
predicted_labels_ids = np.argmax(raw_predictions.predictions, axis=-1)
id2label = model.config.id2label
df_full['predicted_sentiment'] = [id2label[id] for id in predicted_labels_ids]

print("✅ Prediksi sentimen selesai")
print(f"📊 Total tweets yang diprediksi: {len(df_full)}")

# Tampilkan distribusi prediksi
print("\n📊 Distribusi prediksi sentimen:")
prediction_counts = df_full['predicted_sentiment'].value_counts()
print(prediction_counts)

# ==============================================================================
# FASE 5: ANALISIS KORELASI DENGAN VOLATILITAS HARGA
# ==============================================================================
print("\n" + "=" * 60)
print("FASE 5: ANALISIS KORELASI DENGAN VOLATILITAS HARGA")
print("=" * 60)

import yfinance as yf
import statsmodels.api as sm
from scipy.stats import pearsonr
from datetime import datetime

# 5.1 Download Data Harga Bitcoin
print("📈 Mengunduh data harga historis BTC-USD...")
try:
    # Extend the date range to ensure enough data for volatility calculation
    btc_price = yf.download('BTC-USD', start='2023-01-01', end='2024-02-01')

    # Reset index to avoid MultiIndex
    btc_price = btc_price.reset_index()
    btc_price.set_index('Date', inplace=True)

    # Flatten the MultiIndex columns immediately after loading
    if isinstance(btc_price.columns, pd.MultiIndex):
        btc_price.columns = ['_'.join(col).strip() for col in btc_price.columns.values]


    print(f"✅ Data harga berhasil diunduh: {len(btc_price)} hari")
    print(f"📊 Kolom data harga: {list(btc_price.columns)}")

except Exception as e:
    print(f"❌ Error mengunduh data: {e}")

# 5.2 Hitung Volatilitas
print("\n📊 Menghitung volatilitas harga...")

# Ensure btc_price index is a single level (it should be after reset_index)
if isinstance(btc_price.index, pd.MultiIndex):
    btc_price = btc_price.reset_index()
    btc_price.set_index('Date', inplace=True)

# Flatten the MultiIndex columns
if isinstance(btc_price.columns, pd.MultiIndex):
    btc_price.columns = ['_'.join(col).strip() for col in btc_price.columns.values]

# Convert index to datetime objects
btc_price.index = pd.to_datetime(btc_price.index)


# Now access the columns using their flattened names
btc_price['log_return'] = np.log(btc_price['Close_BTC-USD'] / btc_price['Close_BTC-USD'].shift(1))
btc_price['volatility'] = btc_price['log_return'].rolling(window=7).std()
btc_price.dropna(inplace=True)

print(f"✅ Volatilitas berhasil dihitung untuk {len(btc_price)} hari")

# 5.3 Agregasi Sentimen Harian
print("\n📊 Mengagregasi sentimen per hari...")

# Perbaiki parsing timestamp dengan format mixed
df_full['date'] = pd.to_datetime(df_full['timestamp'], format='mixed', utc=True).dt.date
df_full['date'] = pd.to_datetime(df_full['date'])

# Hitung sentiment score harian
daily_sentiment = pd.crosstab(df_full['date'], df_full['predicted_sentiment'])
daily_sentiment['sentiment_score'] = (
    daily_sentiment.get('Bullish', 0) - daily_sentiment.get('Bearish', 0)
) / (
    daily_sentiment.get('Bullish', 0) +
    daily_sentiment.get('Bearish', 0) +
    daily_sentiment.get('Neutral', 0)
)

print(f"✅ Sentimen harian berhasil dihitung untuk {len(daily_sentiment)} hari")

# --- BAGIAN TAMBAHAN: MENGHITUNG SKOR BERBOBOT ---
weight_schema = {'tweet': 1.5, 'quote tweet': 1.2, 'retweet': 1.0}
df_full['weight'] = df_full['tweet_type'].map(weight_schema).fillna(1.0)
daily_sentiment_weighted = df_full.groupby('date').apply(
    lambda x: x.groupby('predicted_sentiment')['weight'].sum()
).unstack(fill_value=0)
daily_sentiment_weighted['sentiment_score_weighted'] = (
    daily_sentiment_weighted.get('Bullish', 0) - daily_sentiment_weighted.get('Bearish', 0)
) / (
    daily_sentiment_weighted.get('Bullish', 0) +
    daily_sentiment_weighted.get('Bearish', 0) +
    daily_sentiment_weighted.get('Neutral', 0)
)
print("✅ Skor sentimen berbobot berhasil dihitung.")

# 5.4 Merge Data Harga dan KEDUA Skor Sentimen
print("\n🔗 Menggabungkan data harga dan kedua skor sentimen...")

# Gabungkan data harga dengan skor sentimen original terlebih dahulu
df_merged = btc_price[['Close_BTC-USD', 'volatility']].merge(
    daily_sentiment[['sentiment_score']],
    left_index=True,
    right_index=True,
    how='inner'
)

# SEKARANG, gabungkan DataFrame di atas dengan skor sentimen berbobot
df_merged = df_merged.merge(
    daily_sentiment_weighted[['sentiment_score_weighted']],
    left_index=True,
    right_index=True,
    how='inner'
)


print(f"✅ Data berhasil digabungkan: {len(df_merged)} hari")
if len(df_merged) > 0:
    print("\n📋 Contoh data gabungan (dengan kedua skor sentimen):")
    print(df_merged.head())
else:
    print("\n⚠️ df_merged kosong. Tidak ada tanggal yang sama untuk digabungkan.")

# 5.5 Analisis Korelasi (Untuk Kedua Skor)
print("\n" + "="*50)
print("ANALISIS KORELASI SENTIMEN DAN VOLATILITAS")
print("="*50)

# Pastikan tidak ada nilai NaN sebelum korelasi
df_merged_corr = df_merged.dropna()

# Korelasi untuk Skor Original
corr_orig, p_orig = pearsonr(df_merged_corr['sentiment_score'], df_merged_corr['volatility'])
print(f"📊 Korelasi Skor Original: {corr_orig:.4f} (p-value: {p_orig:.4f})")

# Korelasi untuk Skor Berbobot
corr_weighted, p_weighted = pearsonr(df_merged_corr['sentiment_score_weighted'], df_merged_corr['volatility'])
print(f"⚖️  Korelasi Skor Berbobot: {corr_weighted:.4f} (p-value: {p_weighted:.4f})")

# 5.6 Analisis Regresi untuk Prediksi (Untuk Kedua Skor)
print("\n" + "="*50)
print("ANALISIS REGRESI PREDIKTIF")
print("="*50)

# --- Model Regresi untuk Skor Original ---
df_merged['sentiment_lagged'] = df_merged['sentiment_score'].shift(1)
df_reg_orig = df_merged.dropna(subset=['volatility', 'sentiment_lagged'])

Y = df_reg_orig['volatility']
X_orig = sm.add_constant(df_reg_orig['sentiment_lagged'])
model_reg_orig = sm.OLS(Y, X_orig).fit()

print("\n📊 Hasil Regresi Linear (Skor Original):")
print(model_reg_orig.summary())


# --- Model Regresi untuk Skor Berbobot ---
df_merged['sentiment_weighted_lagged'] = df_merged['sentiment_score_weighted'].shift(1)
df_reg_weighted = df_merged.dropna(subset=['volatility', 'sentiment_weighted_lagged'])

Y = df_reg_weighted['volatility']
X_weighted = sm.add_constant(df_reg_weighted['sentiment_weighted_lagged'])
model_reg_weighted = sm.OLS(Y, X_weighted).fit()

print("\n\n⚖️  Hasil Regresi Linear (Skor Berbobot):")
print(model_reg_weighted.summary())

# 5.7 Visualisasi Hasil (Dengan Perbandingan Original vs. Berbobot)
print("\n📊 Membuat visualisasi hasil perbandingan...")

plt.figure(figsize=(15, 18))

# --- BARIS 1: PLOT TIME SERIES ---
# Plot 1: Perbandingan Time Series Skor Sentimen
plt.subplot(3, 2, 1)
plt.plot(df_merged.index, df_merged['sentiment_score'], label='Skor Original', alpha=0.8)
plt.plot(df_merged.index, df_merged['sentiment_score_weighted'], label='Skor Berbobot', alpha=0.8, linestyle='--')
plt.ylabel('Skor Sentimen')
plt.title('Perbandingan Skor Sentimen Over Time')
plt.legend()
plt.xticks(rotation=45)

# Plot 2: Time Series Volatilitas (Tetap sama)
plt.subplot(3, 2, 2)
plt.plot(df_merged.index, df_merged['volatility'], label='Volatility', color='red', alpha=0.7)
plt.ylabel('Volatility')
plt.title('Bitcoin Volatility Over Time')
plt.legend()
plt.xticks(rotation=45)

# --- BARIS 2: PLOT KORELASI ---
# Plot 3: Scatter Plot Korelasi (Skor Original)
plt.subplot(3, 2, 3)
plt.scatter(df_merged['sentiment_score'], df_merged['volatility'], alpha=0.5)
plt.xlabel('Skor Sentimen Original')
plt.ylabel('Volatility')
plt.title(f'Korelasi Original: {corr_orig:.4f} (p={p_orig:.4f})')
plt.grid(True, linestyle='--', alpha=0.6)

# Plot 4: Scatter Plot Korelasi (Skor Berbobot)
plt.subplot(3, 2, 4)
plt.scatter(df_merged['sentiment_score_weighted'], df_merged['volatility'], alpha=0.5, color='green')
plt.xlabel('Skor Sentimen Berbobot')
plt.ylabel('Volatility')
plt.title(f'Korelasi Berbobot: {corr_weighted:.4f} (p={p_weighted:.4f})')
plt.grid(True, linestyle='--', alpha=0.6)


# --- BARIS 3: PLOT RESIDUAL DARI REGRESI ---
# Plot 5: Residuals (Skor Original)
plt.subplot(3, 2, 5)
residuals_orig = model_reg_orig.resid
plt.scatter(model_reg_orig.fittedvalues, residuals_orig, alpha=0.5)
plt.xlabel('Fitted Values (Original)')
plt.ylabel('Residuals')
plt.title('Residual Plot (Original)')
plt.axhline(y=0, color='red', linestyle='--')
plt.grid(True, linestyle='--', alpha=0.6)

# Plot 6: Residuals (Skor Berbobot)
plt.subplot(3, 2, 6)
residuals_weighted = model_reg_weighted.resid
plt.scatter(model_reg_weighted.fittedvalues, residuals_weighted, alpha=0.5, color='green')
plt.xlabel('Fitted Values (Berbobot)')
plt.ylabel('Residuals')
plt.title('Residual Plot (Berbobot)')
plt.axhline(y=0, color='red', linestyle='--')
plt.grid(True, linestyle='--', alpha=0.6)

plt.tight_layout()
plt.show()

# ==============================================================================
# FASE 6: KESIMPULAN DAN RINGKASAN (DENGAN PERBANDINGAN)
# ==============================================================================
print("\n" + "=" * 60)
print("KESIMPULAN DAN RINGKASAN PROYEK")
print("=" * 60)

# Siapkan variabel signifikansi untuk kedua hasil
significance_orig = "signifikan secara statistik" if p_orig < 0.05 else "tidak signifikan secara statistik"
significance_weighted = "signifikan secara statistik" if p_weighted < 0.05 else "tidak signifikan secara statistik"

print(f"""
🎯 RINGKASAN HASIL PROYEK:

1. DATA PREPROCESSING:
   ✅ Total tweets diproses: {len(df_full):,}
   ✅ Tweets dengan sentimen valid: {len(df):,}

2. MODEL PERFORMANCE:
   📊 Baseline F1-Score (TF-IDF + Naive Bayes): {baseline_f1_macro:.4f}
   🚀 BERTweet F1-Score (Fine-tuned): {eval_results['eval_f1']:.4f}
   📈 Peningkatan: {eval_results['eval_f1'] - baseline_f1_macro:.4f}

3. ANALISIS PREDIKTIF:
   --- Skor Original ---
   📊 Korelasi Sentimen-Volatilitas: {corr_orig:.4f}
   📈 Signifikansi Statistik: {significance_orig}

   --- Skor Berbobot (Weighted) ---
   ⚖️  Korelasi Sentimen-Volatilitas: {corr_weighted:.4f}
   📈 Signifikansi Statistik: {significance_weighted}

   📅 Periode analisis gabungan: {len(df_merged)} hari

4. KONTRIBUSI ILMIAH:
   ✅ Implementasi BERTweet untuk domain cryptocurrency
   ✅ Analisis komparatif skor sentimen original vs. berbobot
   ✅ Metodologi yang dapat direplikasi untuk aset crypto lainnya

🎉 PROYEK SELESAI DENGAN SUKSES!
""")

print("=" * 60)
print("📝 Model dan hasil analisis telah disimpan di Google Drive")
print("📊 Visualisasi dan metrik evaluasi telah ditampilkan")
print("🔬 Analisis statistik lengkap tersedia untuk publikasi ilmiah")
print("=" * 60)